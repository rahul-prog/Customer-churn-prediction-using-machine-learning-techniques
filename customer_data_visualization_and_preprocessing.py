# -*- coding: utf-8 -*-
"""churn2-checkpoint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vsbpzFmD9p5iqY0I5l7INywR7hMgymtq

<a href="https://colab.research.google.com/github/Paritoshyadav/Minimizing-Churn-Rate-Through-Analysis-of-Financial-Habits/blob/master/Minimizing_Churn_Rate_Through_Analysis_of_Financial_Habits.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

!apt-get update
!apt-get upgrade

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

dataset = pd.read_csv('churn_data.csv')

dataset.head()

dataset.describe()

dataset.isna()

dataset.isna().sum()

dataset = dataset.drop(columns=['credit_score','rewards_earned'])

dataset = dataset[pd.notnull(dataset['age'])]

dataset.isna().sum()

d2=dataset.drop(columns=['user','churn'])

d2.head()

fig=plt.figure(figsize=(5,5))
for i in range(1,d2.shape[1]+1):
  plt.subplot(6,5,i)
  f=plt.gca()
  f.set_title(d2.columns.values[i-1])
  vals=np.size(d2.iloc[:,i-1].unique())
  plt.hist(d2.iloc[:,i-1],bins=vals)
plt.tight_layout(rect=[0,0.83,1,0.95])

dataset[dataset.waiting_4_loan==1].churn.value_counts()

dataset.drop(columns=['churn','user','housing',
                      'payment_type','zodiac_sign']).corrwith(dataset.churn).plot.bar(
figsize=(20,10),title='correlation with response',fontsize=15,rot=45,grid=True)

dataset.to_csv('new_churn.csv',index=False)















corr=dataset.drop(columns=['user','churn']).corr()

mask=np.zeros_like(corr,dtype=np.bool)
mask[np.triu_indices_from(mask)]=True

f,ax=plt.subplots(figsize=(18,15))
cmap=sns.diverging_palette(220,10,as_cmap=True)
sns.heatmap(corr,mask=mask,cmap=cmap,vmax=.3,square=True,linewidth=.5,cbar_kws={"shrink":.5})













sns.distplot(d2['age'], kde=False, rug=True)

"""Same pairplot will take insane amount of time in google colab"""

sns.pairplot(d2)

dataset[d2.waiting_4_loan==1].churn.value_counts()

dataset[d2.cancelled_loan==1].churn.value_counts()

dataset[d2.received_loan==1].churn.value_counts()

dataset[d2.rejected_loan==1].churn.value_counts()

plt.figure(figsize =(30,30))
sns.heatmap(d2.corr(),annot=True,cmap='coolwarm')

dataset=dataset.drop(columns=['app_web_user'])

dataset.to_csv('new',index=False)

id=dataset['user']
dataset=dataset.drop(columns=['user'])

dataset=pd.get_dummies(dataset)
dataset.columns

dataset=dataset.drop(columns=['housing_na','zodiac_sign_na','payment_type_na'])

dataset.head()

from sklearn.model_selection import train_test_split

dataset['churn'].shape

dataset.shape

x_train,x_test,y_train,y_test = train_test_split(dataset.drop(columns=['churn']),dataset['churn'],test_size=0.3,random_state=0,stratify=dataset['churn'])

x_test.shape

x_train.shape

y_train.shape

y_test.shape

y_train.value_counts()

sns.distplot(y_train, bins=20, kde=False, rug=True);

"""some might conside as enough balanced and some might consider balanced
better to not resmaple becoz of its size effect

if u want to balance it
"""

pos=y_train[y_train.values==1]
neg=y_train[y_train.values==0]

pos

if len(pos)>len(neg):
  high = pos
  low = neg
else:
  high=neg
  low=pos

low.value_counts()

from sklearn.utils import resample

"""better to use downsample .... if u think it is imbalance"""

#df_resample = resample(high, replace=False,    # sample without replacement
 #                                n_samples=7814,     # to match minority class
  #                               random_state=123
 #or
"""import random
random.seed(0)
high=np.random.choice(high,size=len(low))
low=np.asarray(low)

new=pd.concat([low,high])
x_train=x_train.loc[new,]
y_train=y_train[new]"""

high.value_counts()

df_resample.value_counts()

new = pd.concat([df_resample, low])

new.value_counts()

#x_train=x_train.loc[new,]
y_train=new

y_train.value_counts()

y_train.shape

x_train.head()

from sklearn.preprocessing import StandardScaler

sc_x = StandardScaler()
xtrain = pd.DataFrame(sc_x.fit_transform(x_train))
xtest=pd.DataFrame(sc_x.fit_transform(x_test))

xtrain.columns = x_train.columns.values

xtrain.head()

xtest.columns = x_test.columns.values

xtest.head()

xtrain.index = x_train.index.values
xtest.index = x_test.index.values

xtrain.corrwith(y_train).plot.bar(figsize=(30,30),title='correlation',fontsize=15,rot=45,grid=True)

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(class_weight="balanced")

rfe=RFE(model,20)

rfe = rfe.fit(xtrain,y_train)

y=rfe.predict(xtest)

xtrain.columns[rfe.support_]

from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
cm=confusion_matrix(y_test,y)

cm

print(classification_report(y_test,y))

accuracy_score(y_test,y)

"""time to improve it"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.svm import SVC

model=SVC()

param_grid = {'C': [0.1,1,5,10,15,18,20,25],'gamma':[0.1,1,0.01,0.2,0.02,0.5,0.3,0.03],'kernel':['rbf']}

from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(model,param_grid,refit=True,verbose=4)

grid.fit(xtrain,y_train)

grid.best_params_

pred=grid.predict(xtest)

pred

cm=confusion_matrix(y_test,y)

cm

accuracy_score(y_test,pred)

print(classification_report(y_test,pred))

"""better result"""

result=SVC(C=15, gamma=0.01, kernel='rbf')

result.fit(xtrain[xtrain.columns[rfe.support_]],y_train)

cm=confusion_matrix(xtest[xtest.columns[rfe.support_]],y_test)

accuracy_score(y_test,result.predict(xtest[xtrain.columns[rfe.support_]]))

"""more better now"""